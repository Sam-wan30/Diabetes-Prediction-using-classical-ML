{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2903050",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Using Machine Learning\n",
    "\n",
    "## IIT Guwahati - Data Science Project\n",
    "\n",
    "**Project Overview:**  \n",
    "This project aims to predict diabetes risk using machine learning algorithms on the Pima Indians Diabetes dataset. We employ Logistic Regression and Linear Discriminant Analysis (LDA) to build predictive models and evaluate their performance.\n",
    "\n",
    "**Dataset:** Pima Indians Diabetes Dataset (768 samples, 8 features)\n",
    "\n",
    "**Algorithms:** Logistic Regression, Linear Discriminant Analysis (LDA)\n",
    "\n",
    "**Objective:** Develop and evaluate classification models to predict diabetes based on patient health metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044024e3",
   "metadata": {},
   "source": [
    "## 1. Title & Introduction\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Diabetes is a chronic metabolic disorder affecting millions worldwide. Early detection and risk assessment are crucial for effective prevention and management. This project develops machine learning models to predict diabetes risk based on clinical measurements.\n",
    "\n",
    "### 1.2 Objectives\n",
    "\n",
    "1. Perform exploratory data analysis to understand the dataset\n",
    "2. Preprocess data to handle missing values and ensure data quality\n",
    "3. Train and evaluate classification models (Logistic Regression and LDA)\n",
    "4. Interpret model coefficients to understand feature importance\n",
    "5. Assess model performance using multiple evaluation metrics\n",
    "\n",
    "### 1.3 Dataset Description\n",
    "\n",
    "The Pima Indians Diabetes dataset contains medical measurements from 768 female patients of Pima Indian heritage. The dataset includes 8 input features and 1 target variable (Outcome: 0 = No Diabetes, 1 = Diabetes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14238348",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading\n",
    "\n",
    "We begin by loading the diabetes dataset and examining its basic structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print()\n",
    "print(\"Column Names:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e429d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefae192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e53fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa835462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "\n",
    "# Check for zero values (which may represent missing data in medical context)\n",
    "print(\"\\nZero values in each column:\")\n",
    "zero_counts = {}\n",
    "for col in df.columns:\n",
    "    if col != 'Outcome':\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_counts[col] = zero_count\n",
    "        print(f\"{col:25s}: {zero_count:4d} zeros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"Target Variable Distribution (Outcome):\")\n",
    "print(df['Outcome'].value_counts())\n",
    "print()\n",
    "print(\"Percentage distribution:\")\n",
    "print(df['Outcome'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['Outcome'].value_counts().plot(kind='bar', color=['#3498db', '#e74c3c'])\n",
    "plt.title('Distribution of Diabetes Outcomes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Outcome (0 = No Diabetes, 1 = Diabetes)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f72f5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We perform comprehensive exploratory data analysis to understand feature distributions, correlations, and relationships with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240992f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by outcome\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "feature_columns = df.columns[:-1]  # All columns except 'Outcome'\n",
    "\n",
    "for idx, col in enumerate(feature_columns):\n",
    "    # Box plot\n",
    "    df.boxplot(column=col, by='Outcome', ax=axes[idx], grid=False)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Outcome', fontsize=10)\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Diabetes Outcome', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            mask=mask)\n",
    "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c150da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with Outcome (target variable)\n",
    "print(\"Correlation with Outcome (Target Variable):\")\n",
    "print(\"-\" * 50)\n",
    "outcome_corr = correlation_matrix['Outcome'].sort_values(ascending=False)\n",
    "for feature, corr in outcome_corr.items():\n",
    "    if feature != 'Outcome':\n",
    "        print(f\"{feature:30s}: {corr:6.3f}\")\n",
    "\n",
    "# Visualize correlation with outcome\n",
    "plt.figure(figsize=(10, 6))\n",
    "outcome_corr_sorted = outcome_corr.drop('Outcome').sort_values(ascending=True)\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in outcome_corr_sorted.values]\n",
    "plt.barh(range(len(outcome_corr_sorted)), outcome_corr_sorted.values, color=colors)\n",
    "plt.yticks(range(len(outcome_corr_sorted)), outcome_corr_sorted.index)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.title('Feature Correlation with Diabetes Outcome', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff9097",
   "metadata": {},
   "source": [
    "### 3.1 Key EDA Insights\n",
    "\n",
    "**Key Findings:**\n",
    "- **Class Distribution**: 65.1% No Diabetes, 34.9% Diabetes (moderately imbalanced)\n",
    "- **Top Correlated Features with Outcome**:\n",
    "  1. Glucose (0.467) - Strongest predictor\n",
    "  2. BMI (0.293) - Strong predictor\n",
    "  3. Age (0.238) - Moderate predictor\n",
    "  4. Pregnancies (0.222) - Moderate predictor\n",
    "- **Data Quality**: No explicit missing values, but zero values in several features likely represent missing data\n",
    "- **Feature Ranges**: Features have vastly different scales (e.g., Insulin: 0-846 vs DiabetesPedigreeFunction: 0.08-2.42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad4719",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Medical datasets often contain zero values that represent missing data rather than actual zero measurements. We identify and handle these invalid zeros by replacing them with NaN and imputing using median values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351963e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing logic\n",
    "# We'll apply the same preprocessing as in preprocessing.py\n",
    "\n",
    "# Define columns where zero values are medically invalid\n",
    "invalid_zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Count zeros before processing\n",
    "print(\"Zero values before preprocessing:\")\n",
    "print(\"-\" * 50)\n",
    "zero_counts_before = {}\n",
    "for col in invalid_zero_cols:\n",
    "    zero_count = (df_cleaned[col] == 0).sum()\n",
    "    zero_counts_before[col] = zero_count\n",
    "    print(f\"{col:25s}: {zero_count:4d} zeros\")\n",
    "\n",
    "print(f\"\\nTotal invalid zeros: {sum(zero_counts_before.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160adf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace invalid zeros with NaN\n",
    "for col in invalid_zero_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].replace(0, np.nan)\n",
    "\n",
    "# Calculate median values for imputation\n",
    "median_values = {}\n",
    "for col in invalid_zero_cols:\n",
    "    median_val = df_cleaned[col].median()\n",
    "    median_values[col] = median_val\n",
    "    print(f\"{col:25s}: Median = {median_val:8.2f}\")\n",
    "\n",
    "# Impute missing values with median\n",
    "for col in invalid_zero_cols:\n",
    "    missing_before = df_cleaned[col].isna().sum()\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(median_values[col])\n",
    "    print(f\"{col:25s}: Imputed {missing_before:4d} missing values\")\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify preprocessing\n",
    "print(\"Verification after preprocessing:\")\n",
    "print(\"-\" * 50)\n",
    "remaining_zeros = {}\n",
    "for col in invalid_zero_cols:\n",
    "    zeros_remaining = (df_cleaned[col] == 0).sum()\n",
    "    remaining_zeros[col] = zeros_remaining\n",
    "    if zeros_remaining == 0:\n",
    "        print(f\"{col:25s}: ✓ No zeros remaining\")\n",
    "    else:\n",
    "        print(f\"{col:25s}: ⚠️  {zeros_remaining} zeros still present\")\n",
    "\n",
    "remaining_nans = df_cleaned[invalid_zero_cols].isna().sum()\n",
    "if remaining_nans.sum() == 0:\n",
    "    print(\"\\n✓ No NaN values remaining - all imputed successfully\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Remaining NaN values: {remaining_nans.sum()}\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_cleaned.to_csv('data/diabetes_cleaned.csv', index=False)\n",
    "print(\"\\n✓ Cleaned dataset saved to 'data/diabetes_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8701959",
   "metadata": {},
   "source": [
    "### 4.1 Preprocessing Summary\n",
    "\n",
    "**Actions Performed:**\n",
    "- Identified invalid zero values in 5 medical features (Glucose, BloodPressure, SkinThickness, Insulin, BMI)\n",
    "- Replaced 652 invalid zeros with NaN\n",
    "- Imputed missing values using median (robust to outliers)\n",
    "- Verified all zeros and NaN values were handled\n",
    "\n",
    "**Why Median Imputation?**\n",
    "- Median is robust to outliers (unlike mean)\n",
    "- Preserves distribution shape better\n",
    "- More appropriate for skewed medical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0381394",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We train two classification models: Logistic Regression and Linear Discriminant Analysis (LDA). Both models require feature scaling for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_cleaned.drop('Outcome', axis=1)\n",
    "y = df_cleaned['Outcome']\n",
    "\n",
    "print(\"Dataset prepared for modeling:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d83406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintains class distribution\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Training set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Percentages: {y_train.value_counts(normalize=True) * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Standardization)\n",
    "# Critical for Logistic Regression and LDA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling applied:\")\n",
    "print(\"✓ Training data scaled (mean≈0, std≈1)\")\n",
    "print(\"✓ Test data scaled using training statistics\")\n",
    "print()\n",
    "print(\"Sample scaled values (first 3 rows):\")\n",
    "X_train_scaled.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b84e1",
   "metadata": {},
   "source": [
    "### 5.1 Why Feature Scaling is Necessary\n",
    "\n",
    "**For Logistic Regression:**\n",
    "- Uses gradient descent optimization\n",
    "- Features with different scales cause slow convergence\n",
    "- Large-scale features dominate the optimization process\n",
    "\n",
    "**For LDA:**\n",
    "- Assumes features have similar variances\n",
    "- Without scaling, features with larger variances dominate\n",
    "- Distance calculations are sensitive to feature scales\n",
    "\n",
    "**Solution:** StandardScaler transforms features to have mean=0 and std=1, ensuring fair treatment of all features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression Model\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Logistic Regression trained successfully\")\n",
    "\n",
    "# Display coefficients\n",
    "print(\"\\nLogistic Regression Coefficients:\")\n",
    "print(\"-\" * 50)\n",
    "lr_coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "for _, row in lr_coefficients.iterrows():\n",
    "    direction = \"↑ Increases\" if row['Coefficient'] > 0 else \"↓ Decreases\"\n",
    "    print(f\"{row['Feature']:25s}: {row['Coefficient']:8.4f} ({direction} diabetes risk)\")\n",
    "\n",
    "print(f\"\\nIntercept: {lr_model.intercept_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA Model\n",
    "lda_model = LinearDiscriminantAnalysis(\n",
    "    solver='svd',\n",
    "    shrinkage=None\n",
    ")\n",
    "\n",
    "print(\"Training LDA...\")\n",
    "lda_model.fit(X_train_scaled, y_train)\n",
    "print(\"✓ LDA trained successfully\")\n",
    "\n",
    "# Display coefficients\n",
    "print(\"\\nLDA Coefficients:\")\n",
    "print(\"-\" * 50)\n",
    "lda_coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lda_model.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(lda_model.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "for _, row in lda_coefficients.iterrows():\n",
    "    direction = \"↑ Increases\" if row['Coefficient'] > 0 else \"↓ Decreases\"\n",
    "    print(f\"{row['Feature']:25s}: {row['Coefficient']:8.4f} ({direction} diabetes risk)\")\n",
    "\n",
    "print(f\"\\nIntercept: {lda_model.intercept_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5ea70",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "We evaluate both models using multiple metrics: accuracy, precision, recall, F1-score, and ROC-AUC. We also generate confusion matrices and ROC curves for comprehensive assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lda_pred = lda_model.predict(X_test_scaled)\n",
    "lda_pred_proba = lda_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Predictions generated for both models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Logistic Regression\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_precision = precision_score(y_test, lr_pred)\n",
    "lr_recall = recall_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "\n",
    "# Calculate metrics for LDA\n",
    "lda_accuracy = accuracy_score(y_test, lda_pred)\n",
    "lda_precision = precision_score(y_test, lda_pred)\n",
    "lda_recall = recall_score(y_test, lda_pred)\n",
    "lda_f1 = f1_score(y_test, lda_pred)\n",
    "lda_auc = roc_auc_score(y_test, lda_pred_proba)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Logistic Regression': [lr_accuracy, lr_precision, lr_recall, lr_f1, lr_auc],\n",
    "    'LDA': [lda_accuracy, lda_precision, lda_recall, lda_f1, lda_auc]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a920579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Diabetes', 'Diabetes'],\n",
    "            yticklabels=['No Diabetes', 'Diabetes'])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontsize=11)\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "# LDA Confusion Matrix\n",
    "lda_cm = confusion_matrix(y_test, lda_pred)\n",
    "sns.heatmap(lda_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['No Diabetes', 'Diabetes'],\n",
    "            yticklabels=['No Diabetes', 'Diabetes'])\n",
    "axes[1].set_title('LDA\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual', fontsize=11)\n",
    "axes[1].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix details\n",
    "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
    "tn, fp, fn, tp = lr_cm.ravel()\n",
    "print(f\"True Negatives (TN):  {tn:3d}\")\n",
    "print(f\"False Positives (FP): {fp:3d}\")\n",
    "print(f\"False Negatives (FN): {fn:3d}\")\n",
    "print(f\"True Positives (TP):  {tp:3d}\")\n",
    "\n",
    "print(\"\\nLDA Confusion Matrix:\")\n",
    "tn, fp, fn, tp = lda_cm.ravel()\n",
    "print(f\"True Negatives (TN):  {tn:3d}\")\n",
    "print(f\"False Positives (FP): {fp:3d}\")\n",
    "print(f\"False Negatives (FN): {fn:3d}\")\n",
    "print(f\"True Positives (TP):  {tp:3d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca97155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_pred_proba)\n",
    "lda_fpr, lda_tpr, _ = roc_curve(y_test, lda_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', \n",
    "         linewidth=2, color='#3498db')\n",
    "plt.plot(lda_fpr, lda_tpr, label=f'LDA (AUC = {lda_auc:.3f})', \n",
    "         linewidth=2, color='#e74c3c')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
    "plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbadc6",
   "metadata": {},
   "source": [
    "### 6.1 Evaluation Summary\n",
    "\n",
    "**Logistic Regression Performance:**\n",
    "- Accuracy: ~73.4%\n",
    "- Precision: ~60.3%\n",
    "- Recall: ~70.4%\n",
    "- F1-Score: ~65.0%\n",
    "- ROC-AUC: ~81.3%\n",
    "\n",
    "**LDA Performance:**\n",
    "- Accuracy: ~70.1%\n",
    "- Precision: ~59.1%\n",
    "- Recall: ~48.1%\n",
    "- F1-Score: ~53.1%\n",
    "- ROC-AUC: ~81.3%\n",
    "\n",
    "**Key Observations:**\n",
    "- Both models achieve similar AUC-ROC scores (~81%), indicating good discriminative ability\n",
    "- Logistic Regression shows better recall (catches more diabetes cases)\n",
    "- LDA shows better precision (fewer false positives)\n",
    "- The choice between models depends on whether we prioritize sensitivity (recall) or specificity (precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f684ec",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation\n",
    "\n",
    "We analyze the Logistic Regression coefficients to understand feature importance and their medical significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a311a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display coefficients sorted by absolute value\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Logistic Regression Coefficients (Sorted by Absolute Value):\")\n",
    "print(\"=\" * 70)\n",
    "print(coefficients_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "coefficients_sorted = coefficients_df.sort_values('Coefficient', ascending=True)\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in coefficients_sorted['Coefficient'].values]\n",
    "plt.barh(range(len(coefficients_sorted)), coefficients_sorted['Coefficient'].values, color=colors)\n",
    "plt.yticks(range(len(coefficients_sorted)), coefficients_sorted['Feature'].values)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Logistic Regression Feature Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18ddf5",
   "metadata": {},
   "source": [
    "### 7.1 Feature Importance Analysis\n",
    "\n",
    "**Top 4 Most Important Features (by absolute coefficient):**\n",
    "\n",
    "1. **Glucose (1.18)** - Most important predictor\n",
    "   - Direct indicator of diabetes (blood sugar level)\n",
    "   - Strongest correlation with outcome (0.467)\n",
    "   - Clinical significance: Primary diagnostic marker\n",
    "\n",
    "2. **BMI (0.71)** - Second most important\n",
    "   - Body Mass Index reflects obesity\n",
    "   - Strong correlation with outcome (0.293)\n",
    "   - Clinical significance: Obesity is a major risk factor for Type 2 diabetes\n",
    "\n",
    "3. **Pregnancies (0.37)** - Third most important\n",
    "   - Number of pregnancies\n",
    "   - Moderate correlation with outcome (0.222)\n",
    "   - Clinical significance: Gestational diabetes history increases future risk\n",
    "\n",
    "4. **DiabetesPedigreeFunction (0.29)** - Fourth most important\n",
    "   - Genetic predisposition indicator\n",
    "   - Weak correlation with outcome (0.174)\n",
    "   - Clinical significance: Family history is a non-modifiable risk factor\n",
    "\n",
    "**Interpretation:**\n",
    "- Positive coefficients increase diabetes risk\n",
    "- Negative coefficients decrease diabetes risk\n",
    "- Larger absolute values indicate stronger influence\n",
    "- Since features are standardized, coefficients are directly comparable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453ae9b",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### 8.1 Project Summary\n",
    "\n",
    "This project successfully developed and evaluated machine learning models for diabetes risk prediction using the Pima Indians Diabetes dataset. Key achievements:\n",
    "\n",
    "1. **Data Quality**: Identified and handled 652 invalid zero values representing missing data\n",
    "2. **Model Performance**: Achieved 81.3% AUC-ROC with both Logistic Regression and LDA\n",
    "3. **Feature Insights**: Identified Glucose and BMI as the most important predictors\n",
    "4. **Clinical Relevance**: Model coefficients align with established medical knowledge\n",
    "\n",
    "### 8.2 Key Findings\n",
    "\n",
    "- **Best Performing Model**: Logistic Regression (73.4% accuracy, 81.3% AUC-ROC)\n",
    "- **Most Important Feature**: Glucose (coefficient: 1.18)\n",
    "- **Data Quality Issue**: Zero values in medical features represent missing data, not actual zeros\n",
    "- **Class Imbalance**: Dataset is moderately imbalanced (65% no diabetes, 35% diabetes)\n",
    "\n",
    "### 8.3 Limitations\n",
    "\n",
    "1. **Dataset Size**: 768 samples may limit model generalization\n",
    "2. **Population Specificity**: Pima Indian population may not generalize to other populations\n",
    "3. **Feature Engineering**: Limited to original features; could benefit from interaction terms\n",
    "4. **Model Complexity**: Linear models may miss non-linear relationships\n",
    "\n",
    "### 8.4 Future Work\n",
    "\n",
    "1. **Advanced Models**: Explore Random Forest, XGBoost, or Neural Networks\n",
    "2. **Feature Engineering**: Create interaction features (e.g., Glucose × BMI)\n",
    "3. **Hyperparameter Tuning**: Optimize model parameters using grid search\n",
    "4. **External Validation**: Test on independent datasets\n",
    "5. **Clinical Integration**: Develop user-friendly interface for healthcare providers\n",
    "\n",
    "### 8.5 Final Remarks\n",
    "\n",
    "The models demonstrate good predictive performance and provide interpretable insights into diabetes risk factors. The alignment between model coefficients and clinical knowledge validates the approach. This work contributes to early diabetes detection and risk assessment, supporting preventive healthcare initiatives.\n",
    "\n",
    "---\n",
    "\n",
    "**Project completed for IIT Guwahati Data Science Course**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
